{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modelling is one of the popular technique in NLP  which is used to determine what topics are present in the given corpus. \n",
    "\n",
    "I will be using <b>Latent Dirichlet Allocation</b> abbreviated as <b>LDA</b> to perform topic modeling.\n",
    "\n",
    "In topic modelling, the order does not matter, hence, <b>Document Term Matrix</b> is used instead of corpus. \n",
    "\n",
    "Process:  DTM --> Gensim(LDA) --> Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\srija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\srija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>be sure to tune in and watch trump on late nig...</td>\n",
       "      <td>2009-05-04 13:54:25</td>\n",
       "      <td>510</td>\n",
       "      <td>917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trump will be on the view tomorrow morning to ...</td>\n",
       "      <td>2009-05-04 20:00:10</td>\n",
       "      <td>34</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trump top ten financial on late show with very...</td>\n",
       "      <td>2009-05-08 08:38:08</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new post celebrity apprentice finale and learn...</td>\n",
       "      <td>2009-05-08 15:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my persona will never be that of a wallflower ...</td>\n",
       "      <td>2009-05-12 09:07:28</td>\n",
       "      <td>1375</td>\n",
       "      <td>1945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content                date  \\\n",
       "0  be sure to tune in and watch trump on late nig... 2009-05-04 13:54:25   \n",
       "1  trump will be on the view tomorrow morning to ... 2009-05-04 20:00:10   \n",
       "2  trump top ten financial on late show with very... 2009-05-08 08:38:08   \n",
       "3  new post celebrity apprentice finale and learn... 2009-05-08 15:40:15   \n",
       "4  my persona will never be that of a wallflower ... 2009-05-12 09:07:28   \n",
       "\n",
       "   retweets  favorites  \n",
       "0       510        917  \n",
       "1        34        267  \n",
       "2        13         19  \n",
       "3        11         26  \n",
       "4      1375       1945  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.read_pickle('data/data_clean.pkl')\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Yearly DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abbas</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abide</th>\n",
       "      <th>abiding</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>abnormally</th>\n",
       "      <th>...</th>\n",
       "      <th>zac</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zee</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abandon  abandoned  abbas  abhor  abide  abiding  ability  abject  able  \\\n",
       "year                                                                            \n",
       "2009        0          0      0      0      0        0        0       0     0   \n",
       "2010        0          0      0      0      0        0        0       0     0   \n",
       "2011        0          2      0      0      0        0        1       0     0   \n",
       "2012        1          3      0      0      0        0        3       1    13   \n",
       "2013        2          8      0      1      0        0        8       0    17   \n",
       "\n",
       "      abnormally  ...   zac  zeal  zee  zero  zimbabwe  zip  zone  zoning  \\\n",
       "year              ...                                                       \n",
       "2009           0  ...     0     0    0     0         0    0     1       0   \n",
       "2010           0  ...     0     0    0     0         0    0     0       0   \n",
       "2011           0  ...     0     0    0     1         0    0     0       0   \n",
       "2012           0  ...     0     0    0    14         0    0     0       0   \n",
       "2013           1  ...     0     1    1    21         0    0     4       0   \n",
       "\n",
       "      zoo  zoom  \n",
       "year             \n",
       "2009    0     0  \n",
       "2010    0     0  \n",
       "2011    0     0  \n",
       "2012    0     0  \n",
       "2013    0     0  \n",
       "\n",
       "[5 rows x 9495 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_yearly = pd.read_pickle('data/dtm_yearly.pkl')\n",
    "dtm_yearly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building term document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>year</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abhor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abide</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "year       2009  2010  2011  2012  2013  2014  2015  2016  2017  2018  2019  \\\n",
       "abandon       0     0     0     1     2     0     1     2     0     3     0   \n",
       "abandoned     0     0     2     3     8     0     1     1     1     5     2   \n",
       "abbas         0     0     0     0     0     0     0     0     1     0     0   \n",
       "abhor         0     0     0     0     1     0     0     0     0     0     0   \n",
       "abide         0     0     0     0     0     1     0     0     0     0     0   \n",
       "\n",
       "year       2020  \n",
       "abandon       2  \n",
       "abandoned     1  \n",
       "abbas         0  \n",
       "abhor         0  \n",
       "abide         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required a term document matrix which is transpose of document term matrix\n",
    "\n",
    "tdm_yearly = dtm_yearly.transpose()\n",
    "tdm_yearly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: With all the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Latent Drichelet Algorithm term document matrix has to be converted into sparse matrix then into a specific gensim corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 3, 0, 2],\n",
       "        [0, 0, 2, ..., 5, 2, 1],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating sparse matrix using term document matrix\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm_yearly)\n",
    "sparse_counts.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating gensim corpus using sparse matrix\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim does require dictionary of all the terms and their respective locations in term-document-matrix\n",
    "\n",
    "cv = pickle.load(open(\"data/cv.pkl\", \"rb\"))\n",
    "id2word = dict((v,k) for k,v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.024*\"great\" + 0.012*\"thank\" + 0.012*\"people\" + 0.011*\"trump\" + 0.010*\"just\" + 0.010*\"president\" + 0.008*\"country\" + 0.007*\"big\" + 0.007*\"news\" + 0.007*\"new\"'),\n",
       " (1,\n",
       "  '0.028*\"trump\" + 0.022*\"great\" + 0.016*\"thanks\" + 0.011*\"president\" + 0.009*\"just\" + 0.009*\"thank\" + 0.008*\"like\" + 0.007*\"good\" + 0.007*\"people\" + 0.007*\"new\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying LDA with 2 topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.031*\"trump\" + 0.024*\"great\" + 0.016*\"thanks\" + 0.012*\"president\" + 0.010*\"thank\" + 0.010*\"just\" + 0.008*\"like\" + 0.007*\"run\" + 0.007*\"people\" + 0.007*\"good\"'),\n",
       " (1,\n",
       "  '0.022*\"great\" + 0.011*\"people\" + 0.009*\"president\" + 0.008*\"just\" + 0.008*\"country\" + 0.008*\"news\" + 0.008*\"thank\" + 0.007*\"big\" + 0.007*\"fake\" + 0.007*\"trump\"'),\n",
       " (2,\n",
       "  '0.025*\"great\" + 0.020*\"thank\" + 0.013*\"trump\" + 0.011*\"people\" + 0.011*\"just\" + 0.008*\"new\" + 0.008*\"make\" + 0.007*\"big\" + 0.007*\"today\" + 0.006*\"crooked\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying LDA with 3 topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"great\" + 0.010*\"tax\" + 0.007*\"people\" + 0.007*\"trump\" + 0.006*\"news\" + 0.006*\"just\" + 0.006*\"president\" + 0.006*\"fake\" + 0.005*\"big\" + 0.005*\"today\"'),\n",
       " (1,\n",
       "  '0.034*\"trump\" + 0.023*\"great\" + 0.014*\"president\" + 0.010*\"thank\" + 0.010*\"just\" + 0.008*\"run\" + 0.008*\"make\" + 0.007*\"new\" + 0.007*\"like\" + 0.007*\"people\"'),\n",
       " (2,\n",
       "  '0.024*\"great\" + 0.012*\"people\" + 0.012*\"thank\" + 0.009*\"just\" + 0.009*\"president\" + 0.008*\"news\" + 0.008*\"country\" + 0.008*\"big\" + 0.007*\"trump\" + 0.007*\"new\"'),\n",
       " (3,\n",
       "  '0.029*\"thanks\" + 0.024*\"great\" + 0.020*\"trump\" + 0.010*\"thank\" + 0.010*\"just\" + 0.009*\"good\" + 0.008*\"like\" + 0.008*\"think\" + 0.008*\"people\" + 0.007*\"president\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying LDA with 4 topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2 - Nouns Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_yearly = pd.read_pickle('data/corpus_yearly.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a text and pull out nouns\n",
    "def nouns(text):\n",
    "    # tag NN is for nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>trump night ten list tonight trump view tomorr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>celebrity apprentice list season tycoon touch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>night jimmy tomorrow night ill announcement af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>interview make filing caucus interview i tomor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>deal nothing i history deal deal hope deal cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>today day rest life warming planet record ice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>president club tonight everybody palm beach cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>year thank family – club fog war explanation f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>year murder rate mayor cant help book race vic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>aid nothing deceit thinking haven help level d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>guy book ” lots enjoy year everyone news media...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>equipment world site … … management dive nobod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             transcript\n",
       "2009  trump night ten list tonight trump view tomorr...\n",
       "2010  celebrity apprentice list season tycoon touch ...\n",
       "2011  night jimmy tomorrow night ill announcement af...\n",
       "2012  interview make filing caucus interview i tomor...\n",
       "2013  deal nothing i history deal deal hope deal cou...\n",
       "2014  today day rest life warming planet record ice ...\n",
       "2015  president club tonight everybody palm beach cl...\n",
       "2016  year thank family – club fog war explanation f...\n",
       "2017  year murder rate mayor cant help book race vic...\n",
       "2018  aid nothing deceit thinking haven help level d...\n",
       "2019  guy book ” lots enjoy year everyone news media...\n",
       "2020  equipment world site … … management dive nobod..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_yearly_n = pd.DataFrame(corpus_yearly.transcript.apply(nouns))\n",
    "corpus_yearly_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbas</th>\n",
       "      <th>ability</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abortion</th>\n",
       "      <th>absence</th>\n",
       "      <th>absentee</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absorb</th>\n",
       "      <th>abu</th>\n",
       "      <th>abundance</th>\n",
       "      <th>...</th>\n",
       "      <th>yucca</th>\n",
       "      <th>zac</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zee</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 6091 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abbas  ability  aboard  abortion  absence  absentee  absolute  absorb  \\\n",
       "2009      0        0       0         0        0         0         0       0   \n",
       "2010      0        0       0         0        0         0         0       0   \n",
       "2011      0        1       0         0        0         0         0       0   \n",
       "2012      0        3       0         0        0         0         0       0   \n",
       "2013      0        8       0         0        0         0         3       0   \n",
       "2014      0        8       1         0        1         0         0       1   \n",
       "2015      0        8       0         1        0         1         1       0   \n",
       "2016      0        3       0         0        0         1         1       0   \n",
       "2017      1        1       1         0        0         0         1       0   \n",
       "2018      0        2       0         0        0         0         1       0   \n",
       "2019      0        0       0         5        0         0         4       0   \n",
       "2020      0        1       0         4        0         2         2       0   \n",
       "\n",
       "      abu  abundance ...   yucca  zac  zeal  zee  zero  zimbabwe  zip  zone  \\\n",
       "2009    0          0 ...       0    0     0    0     0         0    0     1   \n",
       "2010    0          0 ...       0    0     0    0     0         0    0     0   \n",
       "2011    0          0 ...       0    0     0    0     1         0    0     0   \n",
       "2012    0          0 ...       0    0     0    0     1         0    0     0   \n",
       "2013    0          0 ...       0    0     1    1     6         0    0     4   \n",
       "2014    0          0 ...       0    0     0    0     2         0    0     3   \n",
       "2015    2          0 ...       0    1     0    0     2         1    1     1   \n",
       "2016    0          0 ...       0    0     0    0     6         0    0     0   \n",
       "2017    0          0 ...       0    0     0    0     2         0    0     1   \n",
       "2018    0          1 ...       0    0     0    0     3         0    0     1   \n",
       "2019    2          0 ...       0    0     0    0    13         0    0     5   \n",
       "2020    0          0 ...       1    0     0    0     4         0    0     0   \n",
       "\n",
       "      zoning  zoo  \n",
       "2009       0    0  \n",
       "2010       0    0  \n",
       "2011       0    0  \n",
       "2012       0    0  \n",
       "2013       0    0  \n",
       "2014       0    0  \n",
       "2015       0    0  \n",
       "2016       0    0  \n",
       "2017       0    1  \n",
       "2018       0    0  \n",
       "2019       1    0  \n",
       "2020       0    0  \n",
       "\n",
       "[12 rows x 6091 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "add_stop_words = ['best', 'good', 'true', 'man', 'today', 'bad', 'today','trump','great','thanks','thank', 'just','president','people','make','people','new','time','like','think','country','big']\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# re-create a document term matrix with only nouns\n",
    "\n",
    "cv_n = CountVectorizer(stop_words = stop_words)\n",
    "data_cv_n = cv_n.fit_transform(corpus_yearly_n.transcript)\n",
    "dtm_yearly_n = pd.DataFrame(data_cv_n.toarray(), columns = cv_n.get_feature_names())\n",
    "dtm_yearly_n.index = corpus_yearly_n.index\n",
    "dtm_yearly_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gensim corpus\n",
    "corpus_n = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(dtm_yearly_n.transpose()))\n",
    "\n",
    "# creating vocab dict\n",
    "id2word_n = dict((v,k) for k, v in cv_n.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"news\" + 0.011*\"media\" + 0.011*\"border\" + 0.010*\"job\" + 0.009*\"house\" + 0.008*\"state\" + 0.008*\"way\" + 0.007*\"election\" + 0.007*\"vote\" + 0.007*\"crime\"'),\n",
       " (1,\n",
       "  '0.009*\"interview\" + 0.008*\"job\" + 0.008*\"world\" + 0.008*\"golf\" + 0.008*\"night\" + 0.007*\"deal\" + 0.007*\"way\" + 0.007*\"day\" + 0.007*\"apprentice\" + 0.007*\"business\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models\n",
    "\n",
    "lda_n = models.LdaModel(corpus=corpus_n, id2word=id2word_n ,num_topics=2, passes=10)\n",
    "lda_n.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"interview\" + 0.008*\"job\" + 0.008*\"world\" + 0.008*\"night\" + 0.007*\"golf\" + 0.007*\"way\" + 0.007*\"vote\" + 0.007*\"tonight\" + 0.007*\"day\" + 0.007*\"deal\"'),\n",
       " (1,\n",
       "  '0.020*\"news\" + 0.012*\"border\" + 0.011*\"media\" + 0.010*\"job\" + 0.009*\"house\" + 0.008*\"state\" + 0.008*\"way\" + 0.008*\"crime\" + 0.007*\"election\" + 0.007*\"china\"'),\n",
       " (2,\n",
       "  '0.002*\"way\" + 0.002*\"news\" + 0.001*\"media\" + 0.001*\"border\" + 0.001*\"deal\" + 0.001*\"job\" + 0.001*\"day\" + 0.001*\"world\" + 0.001*\"vote\" + 0.001*\"election\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models\n",
    "\n",
    "lda_n = models.LdaModel(corpus=corpus_n, id2word=id2word_n ,num_topics=3, passes=10)\n",
    "lda_n.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"job\" + 0.008*\"world\" + 0.008*\"day\" + 0.008*\"work\" + 0.007*\"business\" + 0.007*\"way\" + 0.007*\"apprentice\" + 0.007*\"money\" + 0.007*\"golf\" + 0.007*\"course\"'),\n",
       " (1,\n",
       "  '0.013*\"interview\" + 0.011*\"night\" + 0.009*\"debate\" + 0.009*\"china\" + 0.008*\"tonight\" + 0.008*\"tomorrow\" + 0.008*\"job\" + 0.007*\"deal\" + 0.007*\"vote\" + 0.007*\"election\"'),\n",
       " (2,\n",
       "  '0.020*\"news\" + 0.013*\"border\" + 0.011*\"media\" + 0.010*\"job\" + 0.010*\"house\" + 0.009*\"state\" + 0.008*\"way\" + 0.008*\"crime\" + 0.008*\"election\" + 0.008*\"china\"'),\n",
       " (3,\n",
       "  '0.010*\"vote\" + 0.009*\"golf\" + 0.008*\"way\" + 0.008*\"course\" + 0.008*\"job\" + 0.008*\"world\" + 0.008*\"business\" + 0.007*\"poll\" + 0.007*\"day\" + 0.007*\"apprentice\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models\n",
    "\n",
    "lda_n = models.LdaModel(corpus=corpus_n, id2word=id2word_n ,num_topics=4, passes=10)\n",
    "lda_n.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 3 - Nouns and Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a text and pull out nouns and adjectives\n",
    "def nouns_adjective(text):\n",
    "    # tag NN is for nouns\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)]\n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>sure trump late night top ten list tonight tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>celebrity apprentice outstanding list season b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>late night jimmy tomorrow night ill big announ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>interview make great filing caucus interview p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>deal nothing i worst history big deal deal hop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>today first day rest life most expensive globa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>president club tonight everybody biggest palm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>happy new year thank great family – club fog w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>new year great murder rate record mayor cant f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>united more aid last nothing deceit thinking s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>good talented guy great new book “ ” lots insi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>equipment military world site … bad new … t ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             transcript\n",
       "2009  sure trump late night top ten list tonight tru...\n",
       "2010  celebrity apprentice outstanding list season b...\n",
       "2011  late night jimmy tomorrow night ill big announ...\n",
       "2012  interview make great filing caucus interview p...\n",
       "2013  deal nothing i worst history big deal deal hop...\n",
       "2014  today first day rest life most expensive globa...\n",
       "2015  president club tonight everybody biggest palm ...\n",
       "2016  happy new year thank great family – club fog w...\n",
       "2017  new year great murder rate record mayor cant f...\n",
       "2018  united more aid last nothing deceit thinking s...\n",
       "2019  good talented guy great new book “ ” lots insi...\n",
       "2020  equipment military world site … bad new … t ma..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_yearly_na = pd.DataFrame(corpus_yearly.transcript.apply(nouns_adjective))\n",
    "corpus_yearly_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbas</th>\n",
       "      <th>ability</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abrupt</th>\n",
       "      <th>absence</th>\n",
       "      <th>absentee</th>\n",
       "      <th>absolute</th>\n",
       "      <th>...</th>\n",
       "      <th>yucca</th>\n",
       "      <th>zac</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zee</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 7648 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      abbas  ability  abject  able  aboard  abortion  abrupt  absence  \\\n",
       "2009      0        0       0     0       0         0       0        0   \n",
       "2010      0        0       0     0       0         0       0        0   \n",
       "2011      0        1       0     0       0         0       0        0   \n",
       "2012      0        3       1    13       0         0       0        0   \n",
       "2013      0        8       0    17       0         0       0        0   \n",
       "2014      0        8       0    16       1         0       0        1   \n",
       "2015      0        8       0    22       1         1       0        0   \n",
       "2016      0        3       0    12       0         0       1        0   \n",
       "2017      1        1       0    10       1         0       0        0   \n",
       "2018      0        2       0    19       0         0       0        0   \n",
       "2019      0        0       0    33       0         5       1        0   \n",
       "2020      0        1       0    11       0         4       0        0   \n",
       "\n",
       "      absentee  absolute ...   yucca  zac  zeal  zee  zero  zimbabwe  zip  \\\n",
       "2009         0         0 ...       0    0     0    0     0         0    0   \n",
       "2010         0         0 ...       0    0     0    0     0         0    0   \n",
       "2011         0         0 ...       0    0     0    0     1         0    0   \n",
       "2012         0         4 ...       0    0     0    0     1         0    0   \n",
       "2013         0         6 ...       0    0     1    1     6         0    0   \n",
       "2014         0         5 ...       0    0     0    0     2         0    0   \n",
       "2015         1         4 ...       0    1     0    0     2         1    1   \n",
       "2016         2         3 ...       0    0     0    0     6         0    0   \n",
       "2017         0         2 ...       0    0     0    0     2         0    0   \n",
       "2018         0         7 ...       0    0     0    0     3         0    0   \n",
       "2019         0         6 ...       0    0     0    0    14         0    0   \n",
       "2020         2         3 ...       1    0     0    0     4         0    0   \n",
       "\n",
       "      zone  zoning  zoo  \n",
       "2009     1       0    0  \n",
       "2010     0       0    0  \n",
       "2011     0       0    0  \n",
       "2012     0       0    0  \n",
       "2013     4       0    0  \n",
       "2014     3       0    0  \n",
       "2015     1       0    0  \n",
       "2016     0       0    0  \n",
       "2017     1       0    1  \n",
       "2018     1       0    0  \n",
       "2019     5       1    0  \n",
       "2020     0       0    0  \n",
       "\n",
       "[12 rows x 7648 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_na = CountVectorizer(stop_words = stop_words)\n",
    "data_cv_na = cv_na.fit_transform(corpus_yearly_na.transcript)\n",
    "dtm_yearly_na = pd.DataFrame(data_cv_na.toarray(), columns = cv_na.get_feature_names())\n",
    "dtm_yearly_na.index = corpus_yearly_na.index\n",
    "dtm_yearly_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gensim corpus\n",
    "corpus_na = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(dtm_yearly_na.transpose()))\n",
    "\n",
    "# creating vocab dict\n",
    "id2word_na = dict((v,k) for k, v in cv_na.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"news\" + 0.010*\"fake\" + 0.008*\"border\" + 0.008*\"media\" + 0.007*\"job\" + 0.007*\"united\" + 0.006*\"house\" + 0.006*\"state\" + 0.006*\"military\" + 0.006*\"way\"'),\n",
       " (1,\n",
       "  '0.007*\"interview\" + 0.006*\"job\" + 0.006*\"world\" + 0.006*\"golf\" + 0.006*\"tonight\" + 0.006*\"night\" + 0.005*\"dont\" + 0.005*\"apprentice\" + 0.005*\"deal\" + 0.005*\"way\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models\n",
    "\n",
    "lda_na = models.LdaModel(corpus=corpus_na, id2word=id2word_na ,num_topics=2, passes=10)\n",
    "lda_na.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"interview\" + 0.006*\"world\" + 0.006*\"golf\" + 0.006*\"job\" + 0.006*\"apprentice\" + 0.005*\"deal\" + 0.005*\"night\" + 0.005*\"dont\" + 0.005*\"course\" + 0.005*\"business\"'),\n",
       " (1,\n",
       "  '0.014*\"news\" + 0.011*\"fake\" + 0.009*\"border\" + 0.008*\"media\" + 0.007*\"united\" + 0.007*\"job\" + 0.007*\"house\" + 0.006*\"military\" + 0.006*\"state\" + 0.006*\"china\"'),\n",
       " (2,\n",
       "  '0.010*\"poll\" + 0.009*\"vote\" + 0.009*\"tonight\" + 0.008*\"tomorrow\" + 0.008*\"media\" + 0.008*\"debate\" + 0.007*\"night\" + 0.006*\"campaign\" + 0.006*\"crowd\" + 0.006*\"speech\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models\n",
    "\n",
    "lda_na = models.LdaModel(corpus=corpus_na, id2word=id2word_na ,num_topics=3, passes=10)\n",
    "lda_na.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"job\" + 0.006*\"world\" + 0.006*\"day\" + 0.006*\"work\" + 0.005*\"apprentice\" + 0.005*\"luck\" + 0.005*\"golf\" + 0.005*\"nice\" + 0.005*\"business\" + 0.005*\"course\"'),\n",
       " (1,\n",
       "  '0.009*\"golf\" + 0.008*\"world\" + 0.008*\"course\" + 0.007*\"apprentice\" + 0.007*\"business\" + 0.007*\"vote\" + 0.006*\"day\" + 0.006*\"way\" + 0.006*\"hotel\" + 0.005*\"job\"'),\n",
       " (2,\n",
       "  '0.008*\"tonight\" + 0.008*\"night\" + 0.008*\"interview\" + 0.007*\"poll\" + 0.007*\"debate\" + 0.007*\"vote\" + 0.006*\"job\" + 0.006*\"tomorrow\" + 0.005*\"dont\" + 0.005*\"china\"'),\n",
       " (3,\n",
       "  '0.014*\"news\" + 0.011*\"fake\" + 0.009*\"border\" + 0.008*\"media\" + 0.007*\"united\" + 0.007*\"job\" + 0.007*\"house\" + 0.006*\"military\" + 0.006*\"state\" + 0.006*\"china\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# models\n",
    "\n",
    "lda_na = models.LdaModel(corpus=corpus_na, id2word=id2word_na ,num_topics=4, passes=10)\n",
    "lda_na.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"news\" + 0.014*\"fake\" + 0.014*\"tax\" + 0.009*\"media\" + 0.008*\"honor\" + 0.008*\"day\" + 0.007*\"election\" + 0.007*\"house\" + 0.006*\"national\" + 0.006*\"military\"'),\n",
       " (1,\n",
       "  '0.000*\"news\" + 0.000*\"way\" + 0.000*\"job\" + 0.000*\"vote\" + 0.000*\"day\" + 0.000*\"media\" + 0.000*\"tonight\" + 0.000*\"deal\" + 0.000*\"dont\" + 0.000*\"real\"'),\n",
       " (2,\n",
       "  '0.012*\"poll\" + 0.011*\"vote\" + 0.011*\"tonight\" + 0.008*\"tomorrow\" + 0.008*\"debate\" + 0.008*\"media\" + 0.008*\"night\" + 0.007*\"campaign\" + 0.007*\"job\" + 0.007*\"way\"'),\n",
       " (3,\n",
       "  '0.000*\"job\" + 0.000*\"news\" + 0.000*\"vote\" + 0.000*\"world\" + 0.000*\"right\" + 0.000*\"day\" + 0.000*\"house\" + 0.000*\"media\" + 0.000*\"fake\" + 0.000*\"night\"'),\n",
       " (4,\n",
       "  '0.014*\"news\" + 0.011*\"fake\" + 0.010*\"border\" + 0.008*\"job\" + 0.008*\"united\" + 0.008*\"media\" + 0.007*\"house\" + 0.007*\"military\" + 0.007*\"china\" + 0.006*\"state\"'),\n",
       " (5,\n",
       "  '0.002*\"direct\" + 0.001*\"chronicle\" + 0.001*\"lieutenant\" + 0.001*\"inexplicable\" + 0.001*\"conner\" + 0.001*\"hysterical\" + 0.001*\"showcase\" + 0.001*\"thoughtful\" + 0.001*\"habitat\" + 0.001*\"tara\"'),\n",
       " (6,\n",
       "  '0.000*\"news\" + 0.000*\"tonight\" + 0.000*\"night\" + 0.000*\"job\" + 0.000*\"right\" + 0.000*\"poll\" + 0.000*\"world\" + 0.000*\"way\" + 0.000*\"vote\" + 0.000*\"dont\"'),\n",
       " (7,\n",
       "  '0.011*\"apprentice\" + 0.010*\"luck\" + 0.008*\"tonight\" + 0.008*\"happy\" + 0.007*\"celebrity\" + 0.007*\"day\" + 0.006*\"job\" + 0.006*\"work\" + 0.006*\"champion\" + 0.005*\"money\"'),\n",
       " (8,\n",
       "  '0.000*\"day\" + 0.000*\"job\" + 0.000*\"news\" + 0.000*\"world\" + 0.000*\"way\" + 0.000*\"happy\" + 0.000*\"work\" + 0.000*\"deal\" + 0.000*\"business\" + 0.000*\"golf\"'),\n",
       " (9,\n",
       "  '0.009*\"interview\" + 0.007*\"golf\" + 0.007*\"world\" + 0.006*\"deal\" + 0.006*\"real\" + 0.006*\"course\" + 0.006*\"job\" + 0.006*\"china\" + 0.006*\"business\" + 0.006*\"dont\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_na = models.LdaModel(corpus=corpus_na, id2word=id2word_na ,num_topics=10, passes=80)\n",
    "lda_na.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.018*\"news\" + 0.014*\"fake\" + 0.014*\"tax\" + 0.009*\"media\" + 0.008*\"honor\" + 0.008*\"day\" + 0.007*\"election\" + 0.007*\"house\" + 0.006*\"national\" + 0.006*\"military\"')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_na.print_topics()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009\n",
      "(5, '0.002*\"direct\" + 0.001*\"chronicle\" + 0.001*\"lieutenant\" + 0.001*\"inexplicable\" + 0.001*\"conner\" + 0.001*\"hysterical\" + 0.001*\"showcase\" + 0.001*\"thoughtful\" + 0.001*\"habitat\" + 0.001*\"tara\"')\n",
      "(7, '0.011*\"apprentice\" + 0.010*\"luck\" + 0.008*\"tonight\" + 0.008*\"happy\" + 0.007*\"celebrity\" + 0.007*\"day\" + 0.006*\"job\" + 0.006*\"work\" + 0.006*\"champion\" + 0.005*\"money\"')\n",
      "(9, '0.009*\"interview\" + 0.007*\"golf\" + 0.007*\"world\" + 0.006*\"deal\" + 0.006*\"real\" + 0.006*\"course\" + 0.006*\"job\" + 0.006*\"china\" + 0.006*\"business\" + 0.006*\"dont\"')\n",
      "\n",
      "2010\n",
      "(7, '0.011*\"apprentice\" + 0.010*\"luck\" + 0.008*\"tonight\" + 0.008*\"happy\" + 0.007*\"celebrity\" + 0.007*\"day\" + 0.006*\"job\" + 0.006*\"work\" + 0.006*\"champion\" + 0.005*\"money\"')\n",
      "\n",
      "2011\n",
      "(4, '0.014*\"news\" + 0.011*\"fake\" + 0.010*\"border\" + 0.008*\"job\" + 0.008*\"united\" + 0.008*\"media\" + 0.007*\"house\" + 0.007*\"military\" + 0.007*\"china\" + 0.006*\"state\"')\n",
      "(9, '0.009*\"interview\" + 0.007*\"golf\" + 0.007*\"world\" + 0.006*\"deal\" + 0.006*\"real\" + 0.006*\"course\" + 0.006*\"job\" + 0.006*\"china\" + 0.006*\"business\" + 0.006*\"dont\"')\n",
      "\n",
      "2012\n",
      "(4, '0.014*\"news\" + 0.011*\"fake\" + 0.010*\"border\" + 0.008*\"job\" + 0.008*\"united\" + 0.008*\"media\" + 0.007*\"house\" + 0.007*\"military\" + 0.007*\"china\" + 0.006*\"state\"')\n",
      "(9, '0.009*\"interview\" + 0.007*\"golf\" + 0.007*\"world\" + 0.006*\"deal\" + 0.006*\"real\" + 0.006*\"course\" + 0.006*\"job\" + 0.006*\"china\" + 0.006*\"business\" + 0.006*\"dont\"')\n",
      "\n",
      "2013\n",
      "(7, '0.011*\"apprentice\" + 0.010*\"luck\" + 0.008*\"tonight\" + 0.008*\"happy\" + 0.007*\"celebrity\" + 0.007*\"day\" + 0.006*\"job\" + 0.006*\"work\" + 0.006*\"champion\" + 0.005*\"money\"')\n",
      "(9, '0.009*\"interview\" + 0.007*\"golf\" + 0.007*\"world\" + 0.006*\"deal\" + 0.006*\"real\" + 0.006*\"course\" + 0.006*\"job\" + 0.006*\"china\" + 0.006*\"business\" + 0.006*\"dont\"')\n",
      "\n",
      "2014\n",
      "(9, '0.009*\"interview\" + 0.007*\"golf\" + 0.007*\"world\" + 0.006*\"deal\" + 0.006*\"real\" + 0.006*\"course\" + 0.006*\"job\" + 0.006*\"china\" + 0.006*\"business\" + 0.006*\"dont\"')\n",
      "\n",
      "2015\n",
      "(2, '0.012*\"poll\" + 0.011*\"vote\" + 0.011*\"tonight\" + 0.008*\"tomorrow\" + 0.008*\"debate\" + 0.008*\"media\" + 0.008*\"night\" + 0.007*\"campaign\" + 0.007*\"job\" + 0.007*\"way\"')\n",
      "(9, '0.009*\"interview\" + 0.007*\"golf\" + 0.007*\"world\" + 0.006*\"deal\" + 0.006*\"real\" + 0.006*\"course\" + 0.006*\"job\" + 0.006*\"china\" + 0.006*\"business\" + 0.006*\"dont\"')\n",
      "\n",
      "2016\n",
      "(2, '0.012*\"poll\" + 0.011*\"vote\" + 0.011*\"tonight\" + 0.008*\"tomorrow\" + 0.008*\"debate\" + 0.008*\"media\" + 0.008*\"night\" + 0.007*\"campaign\" + 0.007*\"job\" + 0.007*\"way\"')\n",
      "(4, '0.014*\"news\" + 0.011*\"fake\" + 0.010*\"border\" + 0.008*\"job\" + 0.008*\"united\" + 0.008*\"media\" + 0.007*\"house\" + 0.007*\"military\" + 0.007*\"china\" + 0.006*\"state\"')\n",
      "\n",
      "2017\n",
      "(0, '0.018*\"news\" + 0.014*\"fake\" + 0.014*\"tax\" + 0.009*\"media\" + 0.008*\"honor\" + 0.008*\"day\" + 0.007*\"election\" + 0.007*\"house\" + 0.006*\"national\" + 0.006*\"military\"')\n",
      "(4, '0.014*\"news\" + 0.011*\"fake\" + 0.010*\"border\" + 0.008*\"job\" + 0.008*\"united\" + 0.008*\"media\" + 0.007*\"house\" + 0.007*\"military\" + 0.007*\"china\" + 0.006*\"state\"')\n",
      "\n",
      "2018\n",
      "(0, '0.018*\"news\" + 0.014*\"fake\" + 0.014*\"tax\" + 0.009*\"media\" + 0.008*\"honor\" + 0.008*\"day\" + 0.007*\"election\" + 0.007*\"house\" + 0.006*\"national\" + 0.006*\"military\"')\n",
      "(2, '0.012*\"poll\" + 0.011*\"vote\" + 0.011*\"tonight\" + 0.008*\"tomorrow\" + 0.008*\"debate\" + 0.008*\"media\" + 0.008*\"night\" + 0.007*\"campaign\" + 0.007*\"job\" + 0.007*\"way\"')\n",
      "(4, '0.014*\"news\" + 0.011*\"fake\" + 0.010*\"border\" + 0.008*\"job\" + 0.008*\"united\" + 0.008*\"media\" + 0.007*\"house\" + 0.007*\"military\" + 0.007*\"china\" + 0.006*\"state\"')\n",
      "\n",
      "2019\n",
      "(4, '0.014*\"news\" + 0.011*\"fake\" + 0.010*\"border\" + 0.008*\"job\" + 0.008*\"united\" + 0.008*\"media\" + 0.007*\"house\" + 0.007*\"military\" + 0.007*\"china\" + 0.006*\"state\"')\n",
      "\n",
      "2020\n",
      "(4, '0.014*\"news\" + 0.011*\"fake\" + 0.010*\"border\" + 0.008*\"job\" + 0.008*\"united\" + 0.008*\"media\" + 0.007*\"house\" + 0.007*\"military\" + 0.007*\"china\" + 0.006*\"state\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_transformed = lda_na[corpus_na]\n",
    "year=2008\n",
    "for a in corpus_transformed:\n",
    "    year=year+1\n",
    "    print(year)\n",
    "    for i in a:\n",
    "        print(lda_na.print_topics()[i[0]])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>\n",
    "<b>Attempt 1 - With all the words</b>\n",
    "* Using all the words does not give results which are good enough. \n",
    "\n",
    "<b>Attempt 2 - Only Nouns</b>\n",
    "* Using just nouns does not give results which are good enough. \n",
    "\n",
    "<b>Attempt 3 - Only Nouns and Adjectives </b>\n",
    "* <b>2009 Topics: </b>\n",
    "    * <b>7:</b> celebrity, money\n",
    "    * <b>9:</b> china, business\n",
    "    \n",
    "* <b>2010 Topics: </b>\n",
    "    * <b>7:</b> luck, celebrity, money\n",
    "    \n",
    "* <b>2011 Topics: </b>\n",
    "    * <b>4:</b> Fake news, China, border, military\n",
    "    * <b>9:</b> china, business\n",
    "    \n",
    "* <b>2012 Topics: </b>\n",
    "    * <b>4:</b> Fake news, China, border, military\n",
    "    * <b>9:</b> china, business\n",
    "    \n",
    "* <b>2013 Topics: </b>\n",
    "    * <b>7:</b> luck, celebrity, money\n",
    "    * <b>9:</b> china, business\n",
    "    \n",
    "* <b>2014 Topics: </b>\n",
    "    * <b>9:</b> china, business\n",
    "    \n",
    "* <b>2015 Topics: </b>\n",
    "    * <b>2:</b> vote, media, campaign\n",
    "    * <b>9:</b> china, business\n",
    "    \n",
    "* <b>2016 Topics: </b>\n",
    "    * <b>2:</b> vote, media, campaign\n",
    "    * <b>4:</b> Fake news, China, border, military\n",
    "    \n",
    "* <b>2017 Topics: </b>\n",
    "    * <b>0:</b> Fake News, tax, media, election, military\n",
    "    * <b>4:</b> Fake news, China, border, military\n",
    "    \n",
    "* <b>2018 Topics: </b>\n",
    "    * <b>0:</b> Fake News, tax, media, election, military\n",
    "    * <b>2:</b> vote, media, campaign\n",
    "    * <b>4:</b> Fake news, China, border, military\n",
    "    \n",
    "* <b>2019 Topics: </b> \n",
    "    * <b>4:</b> Fake news, China, border, military\n",
    "    \n",
    "* <b>2020 Topics: </b>\n",
    "    * <b>4:</b> Fake news, China, border, military"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
